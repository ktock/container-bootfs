# Container-Bootfs ([casync](https://github.com/systemd/casync) and [desync](https://github.com/folbricht/desync) inside.)

## About bootfs
Container Bootfs(bootfs below.) aims to achieve block-level image chunking, de-duplication and lazy-pull execution without any modification on container runtime or registry.
The status of this project is __rough PoC__, and the bootfs is imcompelete as mentioned later.
Currently, we levarage [casync](https://github.com/systemd/casync) and [desync](https://github.com/folbricht/desync) for provisioning rootfs and lazy-pulling image data.

## Context
Now, next generation of OCI image spec is under active discussion!
- https://groups.google.com/a/opencontainers.org/forum/#!topic/dev/icXssT3zQxE
- https://github.com/openSUSE/umoci/issues/256

Roughly speaking, some of points which has been frequentry discussed about current OCI are following.
- Inefficiency of layer-level de-duplication.
- Slow container startup time caused by lack of lazy-pull functionality.
- Lack of seek functionality on tar archive format.

So far, several effective concepts has been proposed.(alphabetical order)
- CernVM-FS Graph Driver Plugin for Docker : [https://cvmfs.readthedocs.io/en/stable/cpt-graphdriver.html](https://cvmfs.readthedocs.io/en/stable/cpt-graphdriver.html)
- FILEgrain : [https://github.com/AkihiroSuda/filegrain](https://github.com/AkihiroSuda/filegrain)
- Slacker : [https://www.usenix.org/node/194431](https://www.usenix.org/node/194431)
- sqashfs and stacker : [https://fosdem.org/2019/schedule/event/containers_atomfs/](https://fosdem.org/2019/schedule/event/containers_atomfs/)
If there are projects should be addded, please let me know.

## Bootfs is aiming...
We aimed to achieve contaiainer image's *block-level de-duplication on store*, *on transfer* and *on execution node* and the *lazy-pull execution* *without any modification on contaienr runtime or registry*.
To achieve this, we developed image converter which generates following data.
- Boot image: Generated Docker image. This image include __boot__ program which sets up the execution environment using casync and desync (both of them are also included in the image) then runs original ENTRYPOINT in the container on boot. Casync provisions original image's rootfs using FUSE with included metadata (aka [caibx](https://github.com/systemd/casync#file-suffixes)). Most of image data will be *pulled lazily* and cached locally, from __Remote store__ on access, by desync process. We use desync's [cache functionailty](https://github.com/folbricht/desync#caching), so if some blobs are on node, desync just use the blobs without pulling remotely, which leads to *block-level de-duplication on transfer*. If you use container's Volume as local cache, the __local cache__ can be shared with several containers on node, then you can achieve *block-level inter-container de-duplication on the node*. This image follows [Docker image spec](https://github.com/moby/moby/blob/master/image/spec/v1.2.md), so you can pull and run this image from container registry in very normal way *without modification on the container runtime or registry*.
- Rootfs blobs : Block-level CDC-chunked image's rootfs blobs. We use casync for chunking. Put this blobs on somewhere cluster-grobal storage (we call it __Remote store__). If you store sets of blobs generated by some containers on the same store, you can acheve *block-level de-duplication on the store*.
![alt converting image](image/architecture01.png)

At runtime, the boot program sets up execution environment, casync provisions original rootfs using FUSE and desync pulls rootfs data lazily from remote store, as mentioned above.
The remote store can be anything desync supports.
In our example, we use SSH server including casync.
See the sample SSH server container's [Dockerfile in this repo](/sample/ssh/Dockerfile) which is quite simple.
```
FROM rastasheep/ubuntu-sshd:latest
RUN apt update -y && apt install -y casync
CMD ["/usr/sbin/sshd", "-D"]
```
![alt runtime architecture](image/architecture02.png)

When you use a volume as the local cahce, you can share it among containers on the node.
The volume needs to be mounted on a specific path (`/rootfs.castr`).
It is useful to keep the volume using [a simple volume-keeper container](/sample/cache/Dockerfile) like below, and share it among containers.
```
FROM busybox:latest
RUN mkdir /rootfs.castr
VOLUME /rootfs.castr
CMD tail -f /dev/null
```
![alt converting image](image/architecture03.png)

## TODO list
Currently, bootfs is __rough PoC__.
So currently, the bootfs is not perfect.
Some of the TODOs are listed below.
- [ ]We need to evaluate bootfs in quantitive ways (__critical!!!__).
- [ ]We cannot use container's *volume* functionality if we haven't made mountpoint placeholder (dummy files or directories) on original rootfs in advance, because provisioned rootfs is read-only and we cannot make the placeholder on runtime .
- [ ]SSH client implimentation is very ad-hoc. Let's say, Desync rely on the system's ssh client and we are using [Dropbear](http://matt.ucc.asn.au/dropbear/dropbear.html) which may be fine. But, we inheriting original rootfs's user information configured in `/etc` (which is including `/etc/passwd` file, etc) without creating bootfs specific one. We also ignore known_hosts checking to achive fast boot time.
- [ ]We cannot pull each blobs from container registry, which means we cannot merge the boot image and the blobs into one container image and put it on a container registry. Because we rely on desync on pulling blobs, which doesn't talk registry API by default. First we need to extend desync to support registry API, and then merge the boot image and blobs using the way like [FILEgrain project](https://github.com/AkihiroSuda/filegrain) proposes.
- [ ]Boot image is heavy. But, this would be shared among all containers on node, thanks to container runtime's native layer-level de-duplication functionality. Maybe We need to create lighter binary which includes functionalities of the boot program's setting-up functionality, casync's rootfs-provisioning functionality and desync's lazy-pull functionality.

## Play with sample.

### Preparation.
We use a volume of the [cache container](/sample/cache/Dockerfile) as node-local blob cache, and a [SSH server container](/sample/ssh/Dockerfile) named ${SSH_SERVER_NAME} as the remote store which use a volume ${SSH_SERVER_STORE}.
We use ${CONVERTER_OUTPUT_DIR} directory to get the converted image by the image converter.
```
LOCAL_CACHE_NAME=node-local-cache
LOCAL_CACHE_STORE=/tmp/node-local-cache
SSH_SERVER_NAME=ssh-casync-server
SSH_SERVER_STORE=/tmp/ssh-casync-server-store
CONVERTER_OUTPUT_DIR=/tmp/converter-output
mkdir ${LOCAL_CACHE_STORE} ${SSH_SERVER_STORE} ${CONVERTER_OUTPUT_DIR}
```
Build the remote store container at `/sample/ssh` and run it.
```
sudo docker build -t ${SSH_SERVER_NAME}:v1 .
sudo docker run --rm -d --network="bridge" \
                     --name ${SSH_SERVER_NAME} \
                     -v ${SSH_SERVER_STORE}:/store \
                     ${SSH_SERVER_NAME}:v1
SSH_SERVER_IP=$(sudo docker inspect ${SSH_SERVER_NAME} --format '{{.NetworkSettings.IPAddress}}')
```
Then build the local cache container at `/sample/cache` and run it.
```
sudo docker build -t ${LOCAL_CACHE_NAME}:v1 .
sudo docker run --rm -d --name ${LOCAL_CACHE_NAME} \
                     -v ${LOCAL_CACHE_STORE}:/rootfs.castr \
                     ${LOCAL_CACHE_NAME}:v1
```

### Build the image converter as container at `/` of this repo.
```
sudo docker build -t mkimage:latest .
```

### Convert images.
```
sudo docker run -i -v /var/run/docker.sock:/var/run/docker.sock \
                   -v ${CONVERTER_OUTPUT_DIR}:/output \
                   mkimage:latest ubuntu:latest ubuntu-converted:latest
```
Then, store the blobs into the remote store container's volume.
```
sudo mv ${CONVERTER_OUTPUT_DIR}/rootfs.castr/* ${SSH_SERVER_STORE}/
```

### Run it.
```
sudo docker run -it --privileged --device /dev/fuse \
                    --volumes-from ${LOCAL_CACHE_NAME} \
                    -e BLOB_STORE=ssh://root@${SSH_SERVER_IP}/store \
                    -e DROPBEAR_PASSWORD=root \
                    ubuntu-converted:latest
```
You can share the local cache among containers by specifying `--volumes-from ${LOCAL_CACHE_NAME}` runtime option.

### Compare how many block-levle blobs are actually pulled lazily.
On boot, the number of cached blobs would be like below.
```
find ${LOCAL_CACHE_STORE} -name *.cacnk | wc -l
96
find ${SSH_SERVER_STORE} -name *.cacnk | wc -l
951
```
The number of blobs in ${LOCAL_CACHE_STORE} will increase on access to the rootfs.
After executing `top` command inside container, the number of cached blobs would increase likely below.
```
find ${LOCAL_CACHE_STORE} -name *.cacnk | wc -l
139
find ${SSH_SERVER_STORE} -name *.cacnk | wc -l
951
```
